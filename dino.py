# -*- coding: utf-8 -*-
"""DINO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e6hpRIiywb2fXVB9mnuLByPPtiJLoXEP
"""

import json
import os
import random

# Load the annotations JSON file
with open('/content/drive/MyDrive/random_sample_mavi_2_gt.json', 'r') as f:
    coco_data = json.load(f)

# Shuffle images for a random split
images = coco_data['images']
random.shuffle(images)

# Split into training and validation sets (80-20 split)
train_images = images[:160]
val_images = images[160:]

# Create a dictionary for each set in COCO format
train_data = {
    "images": train_images,
    "annotations": [ann for ann in coco_data['annotations'] if ann['image_id'] in {img['id'] for img in train_images}],
    "categories": coco_data['categories']
}

val_data = {
    "images": val_images,
    "annotations": [ann for ann in coco_data['annotations'] if ann['image_id'] in {img['id'] for img in val_images}],
    "categories": coco_data['categories']
}

# Save the new JSON files
with open('train_annotations.json', 'w') as f:
    json.dump(train_data, f)
with open('val_annotations.json', 'w') as f:
    json.dump(val_data, f)

print("Dataset split completed. Train and validation files saved as 'train_annotations.json' and 'val_annotations.json'.")

import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Load the annotation file for visualization (using training data as an example)
with open('/content/drive/MyDrive/random_sample_mavi_2_gt.json', 'r') as f:
    data = json.load(f)

# Define a function to visualize bounding boxes
def visualize_bboxes(image_dir, data, num_images=5):
    images = data['images']
    annotations = data['annotations']

    # Dictionary to map image IDs to file paths
    image_id_to_path = {img['id']: img['file_name'] for img in images}

    # Visualize the first few images
    for img_info in images[:num_images]:
        img_id = img_info['id']
        img_path = os.path.join(image_dir, image_id_to_path[img_id])

        # Load the image
        img = cv2.imread(img_path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Plot the image
        fig, ax = plt.subplots(1)
        ax.imshow(img_rgb)

        # Draw bounding boxes
        for ann in annotations:
            if ann['image_id'] == img_id:
                x, y, width, height = ann['bbox']
                rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')
                ax.add_patch(rect)

        plt.axis('off')
        plt.show()

# Example usage: specify the directory containing the images
image_dir = "/content/drive/MyDrive/Pedestrian_dataset"  # Replace with the path to your images folder
visualize_bboxes(image_dir, train_data)

# Commented out IPython magic to ensure Python compatibility.
# Clone DINO repository
!git clone https://github.com/IDEA-Research/DINO.git
# %cd DINO

# Install PyTorch and torchvision
!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html

# Install other required packages
!pip install -r requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# Compile CUDA operators
# %cd models/dino/ops
!python setup.py build install

# Run unit test to ensure successful setup
!python test.py
# %cd ../../../..

# Install FiftyOne
!pip install fiftyone

# Import libraries for FiftyOne and COCO dataset download
import fiftyone as fo
import fiftyone.zoo as foz

# Load the COCO-2017 validation dataset
dataset = foz.load_zoo_dataset("coco-2017", split="validation")

# Launch the FiftyOne App to visualize the dataset
session = fo.launch_app(dataset)

import os

# Define the directory for COCO data
coco_dir = '/content/COCODIR'
os.makedirs(coco_dir, exist_ok=True)

# Download the COCO 2017 train, val, and annotations data
!wget -P {coco_dir} http://images.cocodataset.org/zips/train2017.zip
!wget -P {coco_dir} http://images.cocodataset.org/zips/val2017.zip
!wget -P {coco_dir}/annotations http://images.cocodataset.org/annotations/annotations_trainval2017.zip

# Unzip the files
!unzip {coco_dir}/train2017.zip -d {coco_dir}
!unzip {coco_dir}/val2017.zip -d {coco_dir}
!unzip {coco_dir}/annotations/annotations_trainval2017.zip -d {coco_dir}/annotations

from google.colab import drive
drive.mount('/content/drive')

import os
import shutil

source_path = '/content/DINO'
destination_path = '/content'

# List all items in the source directory
for item in os.listdir(source_path):
    item_path = os.path.join(source_path, item)
    # Check if it's a directory
    if os.path.isdir(item_path):
        # Copy the folder to the destination
        shutil.copytree(item_path, os.path.join(destination_path, item))

# Path to the pre-trained DINO model checkpoint in your Google Drive
checkpoint_path = '/content/drive/MyDrive/checkpoint0033_4scale.pth'

import os
import shutil

# Define your dataset paths
dataset_dir = '/content/drive/MyDrive/Pedestrian_dataset'  # Update with your dataset path
train_dir = '/content/COCODIR/train2017'
val_dir = '/content/COCODIR/val2017'
annotations_dir = '/content/COCODIR/annotations'

# Create directories for train and val images
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)

# List all image files
all_images = os.listdir(dataset_dir)
# Shuffle and split the dataset
train_images = all_images[:160]
val_images = all_images[160:200]

# Move train images
for img in train_images:
    shutil.copy(os.path.join(dataset_dir, img), train_dir)

# Move val images
for img in val_images:
    shutil.copy(os.path.join(dataset_dir, img), val_dir)

# You will also need to update the annotations JSON files accordingly.

checkpoint_path = '/content/drive/MyDrive/checkpoint0011_4scale.pth'  # Update if necessary

!bash /content/DINO/scripts/DINO_eval.sh /content/COCODIR /content/COCODIR/checkpoint0033_4scale.pth

!bash /content/DINO/scripts/DINO_train.sh /content/COCODIR

!bash scripts/DINO_train.sh /content/COCODIR --pretrain_model_path /content/drive/MyDrive/checkpoint0011_4scale.pth --finetune_ignore label_enc.weight class_embed

import fiftyone as fo
import fiftyone.zoo as foz

# Load your validation dataset
dataset = fo.Dataset("/content/drive/MyDrive/Pedestrian_dataset")  # Update to your validation dataset name
session = fo.launch_app(dataset)